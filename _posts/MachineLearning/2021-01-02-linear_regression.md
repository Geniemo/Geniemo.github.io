---
title:  "[Machine Learning] 선형 회귀"
excerpt: "Linear regression"

categories:
  - Machine Learning

toc: true
toc_sticky: true
toc_label: "선형 회귀"

last_modified_at: 2022-01-02
---

## 선형 회귀(Linear Regression)란?

선형 회귀는 머신러닝에서 가장 단순하기도 하면서 대표적인 알고리즘으로,<br>
종속 변수 y와 한 개 이상의 독립 변수 X와의 선형 상관 관계를 모델링하는 회귀분석 기법입니다.

좀 더 쉽게 말하면 주어진 데이터로부터<br>
x와 y의 관계를 가장 잘 나타내는 직선을 그리는 것을 말합니다.

선형회귀는 프로그램에게 답을 알려 주면서 학습을 시키기 때문에 지도 학습 알고리즘에 속합니다.

그럼 선형 회귀에 대해 알아보기 전에 먼저 선형 회귀 용어부터 살펴보겠습니다.

## 선형 회귀 용어

- 학습 데이터: 프로그램을 학습시키기 위해 사용하는 데이터
- 목표 변수: 맞추려고 하는 값 (target variable, output variable이라고도 합니다.)
- 입력 변수: 맞추는 데 사용하는 값 (input variable, feature라고도 합니다.)

보통 데이터를 표현할 때,<br>
학습 데이터의 개수를 m,<br>
i번 데이터의 x를 x<sup>(i)</sup>,<br>
i번 데이터의 y를 y<sup>(i)</sup>이라고 표현합니다.

## 가설 함수

선형 회귀에서 우리가 해야 하는 것은 데이터가 있을 때 최적선을 찾는 것입니다.<br>
우리는 최적선을 찾아내기 위해 다양한 함수를 시도해봐야 할텐데,<br>
이렇게 시도해보는 함수들을 가설 함수(hypothesis function)라고 합니다.

우리가 아는 일반적인 직선의 형태는<br>
$y = ax + b$인데 선형 회귀에서는 일반적으로 사용하는 문자들이 따로 있습니다.<br>
$h(x) = \theta_0 + \theta_{1}x$ 의 형식으로 보통 작성하는데요,<br>
지금은 이해의 편의를 위해서 입력 변수가 하나인 경우를 가정했지만,<br>
실제로는 입력 변수가 여러 개인 경우가 더 많습니다.<br>
그럴 경우에 아래와 같이<br>
$f = ax + by + cz + \dots$의 형태로 쓰면 문자가 너무 많아서 보기에 좋지 않습니다.<br>
따라서, 아래와 같이<br>
$h(x) = \theta_0 + \theta_{1}x_{1} + \theta_{2}x_{2} + \dots$의 형태로 쓰는 게 좋습니다.

그리고 선형 회귀의 목적은 가장 적절한 $\theta$를 찾는 것입니다.<br>
이걸 제대로 표현하기 위해서는 $h$ 밑에 $\theta$를 작게 써서<br>
가설 함수 $h$가 $\theta$에 의해 결정된다고 나타낼 수 있습니다.<br>
$h_{\theta}(x) = \theta_0 + \theta_{1}x_{1} + \theta_{2}x_{2} + \dots$

## 평균 제곱 오차 (MSE)

