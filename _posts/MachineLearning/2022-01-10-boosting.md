---
title:  "[Machine Learning] 에다 부스트"
excerpt: "Adaboost"

categories:
  - Machine Learning

toc: true
toc_sticky: true
toc_label: "에다 부스트"

last_modified_at: 2022-01-10
---

## Boosting

부스팅은 또다른 앙상블 기법입니다.<br>
부스팅 역시 앙상블 기법이므로 여러 개의 모델을 씁니다.

부스팅은 일부러 성능이 안좋은 모델들을 사용합니다.<br>
Bagging과 마찬가지로 각 모델이 조금 다른 데이터를 써서 학습하는데,<br>
Bootstrap처럼 데이터를 임의로 만드는 게 아니라<br>
먼저 만든 모델들이 어떻게 예측을 했냐에 따라 뒤에 만드는 데이터 셋이 결정됩니다.

또한, 모델들의 예측을 종합할 때 단순히 투표를 하는 게 아니라<br>
성능이 좋은 모델의 예측을 더 반영합니다.

## 에다 부스트 (Adaboost)

에다 부스트에서도 수많은 결정 트리들을 만듭니다.<br>
에다 부스트에서는 깊은 결정 트리가 아니라 루트 노드 하나와 분류 노드 두 개를 갖는 얕은 결정 트리를 사용합니다.<br>
이런 식으로 하나의 질문과 그 질문에 대한 답으로 바로 예측을 하는 결정 트리를 나무의 그루터기를 의미하는 stump 라고 합니다.<br>
이런 식으로 만든 stump는 성능이 평균적으로 50%를 조금 넘는 정도로 안좋습니다.

그리고 에다 부스트에서는 부스팅 기법답게 각 모델이 사용하는 데이터 셋을 임의로 만들지 않습니다.<br>
어떤 데이터가 있고, 그걸 바탕으로 스텀프를 만들어서 분류를 했다고 하겠습니다.<br>
그럼 올바르게 분류된 데이터들이 있고, 틀리게 분류된 데이터들이 있겠죠?<br>
이것을 바탕으로 다음 스텀프에 쓸 데이터 셋을 만들 때에는<br>
올바르게 분류된 데이터들의 중요도를 낮추고, 틀리게 분류된 데이터들의 중요도를 높입니다.<br>
중요도가 높은 데이터들은 뒤에 만든 스텀프가 우선적으로 맞출 수 있게 합니다.

이런 식으로 각 스텀프는 전 스텀프의 실수를 바로잡는 방향으로 만들어지게 됩니다.

이렇게 수많은 스텀프들을 만든 후에는 종합적인 예측을 해야합니다.<br>
에다 부스트는 다수결의 원칙이 아니라 성능주의적 예측으로 합니다.<br>
예측을 종합할 때 성능이 좋은 스텀프의 의견 비중을 더 높게 반영한다는 것입니다.

## 스텀프 성능 계산하기

에다 부스트는 예측을 종합할 때 각 트리의 성능을 반영하기 때문에 트리를 만들 때마다 성능을 미리 계산해야 합니다.<br>
특정 스텀프의 성능은 아래와 같이 계산합니다.

\\[ \displaystyle \frac{1}{2}log(\frac{1 - total_error}{total_error}) \hbox{, total_error: 잘못 분류한 데이터들의 중요도의 합} \\]

그래프는 아래와 같습니다.

<script src="https://gist.github.com/Geniemo/a2d156482bf727f62b1e4aa393f4be0a.js"></script>

위 식은 \\(total\_error\\)가 1에 가까워질수록 작아지고, 0에 가까워질수록 커집니다.<br>
다른 말로 하면, 특정 스텀프의 성능은 \\(total\_error\\)가 1에 가까워질수록 안좋고, 0에 가까워질수록 좋다는 것입니다.

## 데이터 중요도 바꾸기

다음 스텀프가 학습할 데이터를 만들 때에는<br>
틀리게 분류한 데이터의 중요도를 높여주고,<br>
맞게 분류한 데이터의 중요도를 낮춰줘야 합니다.

틀리게 분류한 모든 데이터에 대해서는 아래와 같이 중요도를 바꿔줍니다.

\\[ weight_{new} = weight_{old} \times e^{P_{tree}}, \hbox{P_{tree}: 스텀프의 성능} \\]

제대로 분류한 모든 데이터에 대해서는 아래와 같이 중요도를 바꿔줍니다.

\\[ weight_{new} = weight_{old} \times e^{-P_{tree}}, \hbox{P_{tree}: 스텀프의 성능} \\]

이렇게 바꿔줬을 때 마지막으로 까먹으면 안되는 게 있습니다.<br>
중요도는 항상 다 더해서 1이 되어야 하므로 각 데이터의 중요도를 모든 중요도의 합으로 나눠줘서 비율이 조절되게 하는 것입니다.

## 스텀프 추가하기

중요도가 바뀐 데이터셋을 이용해서 스텀프들을 추가하는 방법을 알아보겠습니다.

아래와 같은 데이터가 있다고 하겠습니다.

|데이터|중요도|
|-----|------|
|D1|0.1|
|D2|0.2|
|D3|0.3|
|D4|0.4|

새로 만들 데이터 셋은 기존의 데이터 셋과 크기가 같습니다.<br>
각 데이터의 중요도를 새 데이터 셋에 들어갈 확률로 써서 만듭니다.

이를 위해서 각 데이터마다 중요도를 이용해서 범위를 잡아줍니다.<br>
D1은 0 ~ 0.1<br>
D2는 0.1 ~ 0.3<br>
D3는 0.3 ~ 0.6<br>
D4는 0.6 ~ 0.10<br>
이렇게 범위를 잡아주고 나서 0과 1 사이의 임의의 숫자를 고릅니다.<br>
만약 0.7234를 골랐다면 D4를 새 데이터 셋에 추가하는 것이죠.<br>
이 과정에서 중요도가 높은 데이터셋은 범위가 넓기 때문에 더 높은 확률로 선택되게 됩니다.<br>
이런 식으로 원래 데이터 셋과 크기가 같아질 때까지 계속 추가합니다.

이제 이 데이터 셋을 써서 첫 스텀프를 만들 때와 같이 새 스텀프를 만듭니다.<br>
스텀프로 분류를 한 다음 이 스텀프의 성능을 계산합니다.<br>
그 후에는 원래 데이터 셋에 이번에 분류한 결과를 반영합니다.<br>
(틀린 데이터의 중요도를 높이고, 맞은 데이터의 중요도를 낮춥니다.)

이런 식으로 계속 반복하여 스텀프를 추가해 나갈 수 있습니다.

## 예측하기

앞서 예측을 종합할 때 성능이 더 좋은 스텀프의 의견 비중을 더 높게 반영한다고 했는데요,<br>
이걸 예시를 들어 설명해보겠습니다.

스텀프를 4개 만들었다고 하겠습니다.<br>
성능이 0.1인 스텀프, 0.2인 스텀프, 0.3인 스텀프가 C를 예측했다고 하겠습니다.<br>
성능이 가장 높은 0.7인 스텀프는 ~C를 예측했습니다.

이럴 경우에 C를 예측하는 성능의 합은 0.6이고<br>
~C를 예측하는 성능의 합은 0.7입니다.

이럴 경우에 최종 결정은 성능의 합이 높은 쪽으로 따라가게 되어 결국 ~C로 예측하게 된다는 것입니다.

## scikit-learn으로 에다 부스트 써보기

<script src="https://gist.github.com/Geniemo/90be6754300a75c9ee81484e3a3e2c3d.js"></script>