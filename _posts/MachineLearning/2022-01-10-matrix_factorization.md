---
title:  "[Machine Learning] 행렬 인수분해"
excerpt: "Matrix Factorization"

categories:
  - Machine Learning

toc: true
toc_sticky: true
toc_label: "행렬 인수분해"

last_modified_at: 2022-01-10
---

## 행렬 인수분해

사람들의 각 항목에 대한 기호 정보가 있는 행렬이 있을 때,<br>
이 행렬은 두 개의 행렬의 곱으로 나타낼 수 있습니다.

이를 이용한 예측이 가능한데,<br>
예를 들어서 아래와 같이 몇 칸이 비어있는 행렬이 있다고 하겠습니다.

|      |항목 1|항목 2|항목 3|
|------|------|-----|--------|
|사람 1|4|1|5|
|사람 2|2|?|3|
|사람 3|?|3|1|

몇 칸이 비어있기 때문에 곱했을 때 완벽히 똑같은 두 개의 행렬은 구하기 힘들 수도 있습니다.<br>
따라서 이 행렬을 인수분해해서 곱했을 때 가장 가까운 두 개의 행렬의 곱으로 나타낸다면,<br>
두 행렬의 곱으로 빈 칸의 값을 예측할 수 있습니다.

## 속성 학습

여기서 속성을 학습할 때에도 경사 하강법을 이용합니다.<br>
먼저 두 행렬에 임의의 값을 넣고 곱해서 목표 행렬과 얼마나 차이가 나는지 구합니다.<br>
여기서 차이를 구할 때 어떻게 차이를 구할건지에 대한 기준이 필요한데요,<br>
그 기준으로 손실 함수가 있습니다.<br>
여기서의 손실 함수는 아래와 같습니다.

\\[ \displaystyle J(\Theta, X) = \frac{1}{2}\sum_{i,j:r(i,j)=1}((\theta^{(i)})^{T}x^{(j)} - y^{(i,j)})^{2} \\]

정리하면 각 예측값마다 제곱 오차를 구하는데 이걸 비어있지 않은 데이터에 대해서만 구해서 더해준다는 말입니다.<br>
즉, 전체 데이터에 대한 제곱 오차 합을 계산하는 것입니다.

## 경사 하강법

여기서도 경사 하강법을 사용할 때 손실 함수를 줄여주는 방향으로 입력 변수들을 조정해나가야 합니다.<br>
앞서 다뤘던 경사 하강법과 차이가 있다면 입력 변수가 \\(\theta\\)뿐만 아니라 \\(x\\)까지 있다는 것입니다.

이걸 고려하면 수식은 아래와 같이 됩니다.

모든 i, j, k에 대하여

\\[ \displaystyle \theta_{k}^{(i)} \leftarrow \theta_{k}^{(i)} - \alpha\frac{\partial}{\partial\theta_{k}^{(i)}}J(\Theta, X) = \theta_{k}^{(i)} - \alpha\sum_{j:r(i, j)=1}((\theta^{(i)})^{T}x^{(j)} - y^{(i, j)})x_{k}^{(j)} \\]

\\[ \displaystyle x_{k}^{(j)} \leftarrow x_{k}^{(j)} - \alpha\frac{\partial}{\partial x_{k}^{(j)}}J(\Theta, X) = x_{k}^{(j)} - \alpha\sum_{j:r(i, j)=1}((\theta^{(i)})^{T}x^{(j)} - y^{(i, j)})\theta_{k}^{(i)} \\]

이를 이용하여 경사 하강법을 한 번 할 때마다 \\(\theta\\)와 \\(X\\)를 모두 업데이트 해주는 것입니다.

이렇게 경사 하강법을 마치고 나면 예측은 쉽습니다.<br>
학습한 데이터를 사용해서 i행 j열의 기호 정보는 아래와 같은 수식으로 표현할 수 있습니다.

\\[ \displaystyle (\theta^{(i)})^{T}x^{(j)} \\]

## 손실 함수 볼록도

행렬 인수분해의 손실 함수는 아쉽게도 아래로 볼록하지 않습니다.<br>
선형 회귀와는 달리 변수가 두 개가 있고 그 둘이 곱해졌기 때문입니다.

따라서 임의로 값들을 초기화하고 경사 하강법을 적용해도 손실을 가장 작게 만드는 값을 찾았다고 장담할 수는 없습니다.<br>
따라서 이런 문제점을 극복하기 위해서 임의로 초기화를 여러번 해서 경사 하강법을 많이 해본 이후,<br>
가장 성능이 좋게 나온 모델을 쓸 수 있습니다.

## 행렬 인수분해 정규화

행렬 인수분해를 할 때도 과대적합 문제가 발생할 수 있습니다.<br>
앞서 다뤘던 것과 같이 정규화 항을 더해줌으로써 문제를 해결할 수 있습니다.<br>
이번에는 변수가 두 개이므로 L1 정규화를 하려면 아래의 손실 함수를 사용하고,<br>
(\\( n_{u} \\))는 유저 데이터 수, (\\( n_{item} \\))은 항목의 수, (\\( n \\))은 속성 개수라고 하겠습니다.)

\\[ \displaystyle J(\Theta, X) = \frac{1}{2}\sum_{i,j:r(i,j)=1}((\theta^{(i)})^{T}x^{(j)} - y^{(i,j)})^{2} + \frac{\lambda}{2}\sum_{i=1}^{n_{u}}\sum_{k=1}^{n} \vert \theta_{k}^{(i)} \vert + \frac{\lambda}{2}\sum_{j=1}^{n_{item}}\sum_{k=1}^{n} \vert x_{k}^{(j)} \vert \\]

L2 정규화를 하려면 아래의 손실 함수를 사용합니다.

\\[ \displaystyle J(\Theta, X) = \frac{1}{2}\sum_{i,j:r(i,j)=1}((\theta^{(i)})^{T}x^{(j)} - y^{(i,j)})^{2} + \frac{\lambda}{2}\sum_{i=1}^{n_{u}}\sum_{k=1}^{n} (\theta_{k}^{(i)})^{2} + \frac{\lambda}{2}\sum_{j=1}^{n_{item}}\sum_{k=1}^{n} (x_{k}^{(j)})^{2} \\]